#实习复习资料

##GDAL（RasterIO）

	GDAL(Geospatial Data Abstraction Library)是一个在X/MIT许可协议下的开源栅格空间数据转换库。它利用抽象数据模型来表达所支持的各种文件格式。

关于RasterIO
	在GDAL中读写图像是最基本的操作，那么RasterIO也就是最基本的函数了，关于RasterIO有很多方式，这个函数的功能相当强大，下面慢慢说明。RasterIO一共有两个，一个是GDALRasterBand::RasterIO，另一个是GDALDataset::RasterIO，这两个RasterIO都可以对图像数据来进行读写，大多数情况下是一样的，但是还是有一些区别的。
	

##HOG：
	
###1、HOG特征：

       方向梯度直方图（Histogram of Oriented Gradient, HOG）特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。Hog特征结合SVM分类器已经被广泛应用于图像识别中，尤其在行人检测中获得了极大的成功。需要提醒的是，HOG+SVM进行行人检测的方法是法国研究人员Dalal在2005的CVPR上提出的，而如今虽然有很多行人检测算法不断提出，但基本都是以HOG+SVM的思路为主。

（1）主要思想：

       在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。（本质：梯度的统计信息，而梯度主要存在于边缘的地方）。

（2）具体的实现方法是：

       首先将图像分成小的连通区域，我们把它叫细胞单元。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。

（3）提高性能：

       把这些局部直方图在图像的更大的范围内（我们把它叫区间或block）进行对比度归一化（contrast-normalized），所采用的方法是：先计算各直方图在这个区间（block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。通过这个归一化后，能对光照变化和阴影获得更好的效果。

（4）优点：

       与其他的特征描述方法相比，HOG有很多优点。首先，由于HOG是在图像的局部方格单元上操作，所以它对图像几何的和光学的形变都能保持很好的不变性，这两种形变只会出现在更大的空间领域上。其次，在粗的空域抽样、精细的方向抽样以及较强的局部光学归一化等条件下，只要行人大体上能够保持直立的姿势，可以容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。因此HOG特征是特别适合于做图像中的人体检测的。

 

###2、HOG特征提取算法的实现过程：

大概过程：

HOG特征提取方法就是将一个image（你要检测的目标或者扫描窗口）：

1）灰度化（将图像看做一个x,y,z（灰度）的三维图像）；

2）采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）；目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰；

3）计算图像每个像素的梯度（包括大小和方向）；主要是为了捕获轮廓信息，同时进一步弱化光照的干扰。

4）将图像划分成小cells（例如6*6像素/cell）；

5）统计每个cell的梯度直方图（不同梯度的个数），即可形成每个cell的descriptor；

6）将每几个cell组成一个block（例如3*3个cell/block），一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor。

7）将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image（你要检测的目标）的HOG特征descriptor了。这个就是最终的可供分类使用的特征向量了。

 

 

具体每一步的详细过程如下：

（1）标准化gamma空间和颜色空间

     为了减少光照因素的影响，首先需要将整个图像进行规范化（归一化）。在图像的纹理强度中，局部的表层曝光贡献的比重较大，所以，这种压缩处理能够有效地降低图像局部的阴影和光照变化。因为颜色信息作用不大，通常先转化为灰度图；

     Gamma压缩公式：

     比如可以取Gamma=1/2；

 

（2）计算图像梯度

        计算图像横坐标和纵坐标方向的梯度，并据此计算每个像素位置的梯度方向值；求导操作不仅能够捕获轮廓，人影和一些纹理信息，还能进一步弱化光照的影响。

图像中像素点(x,y)的梯度为：

       最常用的方法是：首先用[-1,0,1]梯度算子对原图像做卷积运算，得到x方向（水平方向，以向右为正方向）的梯度分量gradscalx，然后用[1,0,-1]T梯度算子对原图像做卷积运算，得到y方向（竖直方向，以向上为正方向）的梯度分量gradscaly。然后再用以上公式计算该像素点的梯度大小和方向。

 

（3）为每个细胞单元构建梯度方向直方图

        第三步的目的是为局部图像区域提供一个编码，同时能够保持对图像中人体对象的姿势和外观的弱敏感性。

我们将图像分成若干个“单元格cell”，例如每个cell为6*6个像素。假设我们采用9个bin的直方图来统计这6*6个像素的梯度信息。也就是将cell的梯度方向360度分成9个方向块，如图所示：例如：如果这个像素的梯度方向是20-40度，直方图第2个bin的计数就加一，这样，对cell内每个像素用梯度方向在直方图中进行加权投影（映射到固定的角度范围），就可以得到这个cell的梯度方向直方图了，就是该cell对应的9维特征向量（因为有9个bin）。

        像素梯度方向用到了，那么梯度大小呢？梯度大小就是作为投影的权值的。例如说：这个像素的梯度方向是20-40度，然后它的梯度大小是2（假设啊），那么直方图第2个bin的计数就不是加一了，而是加二（假设啊）。

         细胞单元可以是矩形的（rectangular），也可以是星形的（radial）。

 

（4）把细胞单元组合成大的块（block），块内归一化梯度直方图

       由于局部光照的变化以及前景-背景对比度的变化，使得梯度强度的变化范围非常大。这就需要对梯度强度做归一化。归一化能够进一步地对光照、阴影和边缘进行压缩。

        作者采取的办法是：把各个细胞单元组合成大的、空间上连通的区间（blocks）。这样，一个block内所有cell的特征向量串联起来便得到该block的HOG特征。这些区间是互有重叠的，这就意味着：每一个单元格的特征会以不同的结果多次出现在最后的特征向量中。我们将归一化之后的块描述符（向量）就称之为HOG描述符。

        区间有两个主要的几何形状——矩形区间（R-HOG）和环形区间（C-HOG）。R-HOG区间大体上是一些方形的格子，它可以有三个参数来表征：每个区间中细胞单元的数目、每个细胞单元中像素点的数目、每个细胞的直方图通道数目。

       例如：行人检测的最佳参数设置是：3×3细胞/区间、6×6像素/细胞、9个直方图通道。则一块的特征数为：3*3*9；

 

（5）收集HOG特征

      最后一步就是将检测窗口中所有重叠的块进行HOG特征的收集，并将它们结合成最终的特征向量供分类使用。

    

（6）那么一个图像的HOG特征维数是多少呢？

        顺便做个总结：Dalal提出的Hog特征提取的过程：把样本图像分割为若干个像素的单元（cell），把梯度方向平均划分为9个区间（bin），在每个单元里面对所有像素的梯度方向在各个方向区间进行直方图统计，得到一个9维的特征向量，每相邻的4个单元构成一个块（block），把一个块内的特征向量联起来得到36维的特征向量，用块对样本图像进行扫描，扫描步长为一个单元。最后将所有块的特征串联起来，就得到了人体的特征。例如，对于64*128的图像而言，每8*8的像素组成一个cell，每2*2个cell组成一个块，因为每个cell有9个特征，所以每个块内有4*9=36个特征，以8个像素为步长，那么，水平方向将有7个扫描窗口，垂直方向将有15个扫描窗口。也就是说，64*128的图片，总共有36*7*15=3780个特征。

HOG维数，16×16像素组成的block，8x8像素的cell

 

注释：
行人检测HOG+SVM

总体思路：
1、提取正负样本hog特征
2、投入svm分类器训练，得到model
3、由model生成检测子
4、利用检测子检测负样本，得到hardexample
5、提取hardexample的hog特征并结合第一步中的特征一起投入训练，得到最终检测子。

深入研究hog算法原理：
一、hog概述
Histograms of Oriented Gradients,顾名思义，方向梯度直方图，是目标的一种描述的方式，既是描述子。
二、hog提出
hog是05年一位nb的博士提出来的，论文链接 http://wenku.baidu.com/view/676f2351f01dc281e53af0b2.html
三、算法理解
        终于到10月了，终于可以松一口气了，整理一下hog的算法流程。
首先要有一个整体的认识，每一个目标都对应一个一维特征向量，这个向量一共有n维，这个n不是凭空瞎猜的，是有理有据，打个比方，为什么opencv自带的hog检测子是3781维的？这个问题在初期确实比较头疼，纠结了好长的时间，不过别着急，
我们先来看一下opencv里的HOGDescriptor这个结构的构造函数HOGDescriptor（Size winSize,Size blocksize,Size blockStride,Size cellSize,...(后面的参数在这里用不到)），去查一下opencv默认的参数我们可以看到，winSize（64,128），blockSize（16,16），blockStride（8,8），cellSize（8,8），很显然hog是将一个特征窗口win划分为很多的块block，在每一个块里又划分为很多的细胞单元cell(即胞元)，hog特征向量既是把这些所有的cell对应的小特征串起来得到一个高维的特征向量，那么这个窗口对应的一维特征向量维数n就等于窗口中的块数 x 块中的胞元数  x 每一个胞元对应的特征向量数。
写到这里，我们计算一下3781如何得到的，窗口大小64x128，块大小16x16，块步长8x8，那么窗口中块的数目是(（64-16）/8+1)*((128-16)/8+1) = 7*15 =105个块，块大小为16x16,胞元大小为8x8，那么一个块中的胞元cell数目是 (16/8)*(16/8) =4个胞元，到这里我们可以看到要求最后需要的维数n，只需要计算每一个胞元对应的向量，这个参数在哪呢？别急，我们把每一个胞元投影到9个bin（如何投影？这里卡了很长一段时间，后面会说），那么每一个胞元对应的向量就是9维，每个bin对应该9维向量的一个数，现在看一下是不是计算窗口维数的三个需求量都知道了，n = 窗口中的块数 x 块中的胞元数  x 每一个胞元对应的特征向量数,带入看一下n= 105x4x9 = 3780,这就是这个窗口对应的特征了。有人会说，为什么opencv里的getDefaultPeopleDetector()得到的是3781维呢？这是因为另外一维是一维偏移，（很崩溃是吧，我也崩溃很久。。。，下一段解释）。
我们利用hog+svm检测行人，最终的检测方法是最基本的线性判别函数，wx + b = 0，刚才所求的3780维向量其实就是w，而加了一维的b就形成了opencv默认的3781维检测算子，而检测分为train和test两部分，在train期间我们需要提取一些列训练样本的hog特征使用svm训练最终的目的是为了得到我们检测的w以及b，在test期间提取待检测目标的hog特征x，带入方程是不是就能进行判别了呢？

写到这里，至少对hog的运作流程有了一个大概的认识，在网上能看到很多的hog计算方法，神马归一化，计算梯度，对每个胞元进行投影，千篇一律，对刚开始接触的人来说，看完好像懂了，但就是不知道怎么用，hog和svm如何配合，而且那些东西对我们的初期的学期完全没用，好处就是会用hog了，再回过头去看原理，才有收获，那些资料网上一堆，这里就不画蛇添足了。
另外值得一提的是在计算胞元特征的时候，需要向各个bin投影，这个投影里面大有文章，师兄毕业论文里就提到了，取名叫‘三维一次线性插值’，如果想深入了解hog的可以仔细琢磨去。

下面说一下libsvm和CvSVM的使用，我觉得libsvm更好用，不过cvsvm也是基于libsvm2.6(没记错的话)改写的，这两个的区别就是libsvm训练得到的是一个model，而cvsvm是xml文件，在计算最后的wx+b=0中的w向量的时候，对于libsvm直接处理model文件即可，但是对于cvsvm则可以跳过产生xml文件，直接使用cvsvm的对象中的属性即可（这里说的有点模糊，二者选一个即可，关系倒不是很大）
	

##SVM：




##Word Embedding

word2vec:

###一，词向量的概念
  将 word映射到一个新的空间中，并以多维的连续实数向量进行表示叫做“Word Represention” 或 “Word Embedding”。自从21世纪以来，人们逐渐从原始的词向量稀疏表示法过渡到现在的低维空间中的密集表示。用稀疏表示法在解决实际问题时经常会遇到维数灾难，并且语义信息无法表示，无法揭示word之间的潜在联系。而采用低维空间表示法，不但解决了维数灾难问题，并且挖掘了word之间的关联属性，从而提高了向量语义上的准确度。

###二，词向量模型

a)  LSA矩阵分解模型

采用线性代数中的奇异值分解方法，选取前几个比较大的奇异值所对应的特征向量将原矩阵映射到低维空间中，从而达到词矢量的目的。

b)  PLSA 潜在语义分析概率模型

从概率学的角度重新审视了矩阵分解模型，并得到一个从统计，概率角度上推导出来的和LSA相当的词矢量模型。

c)  LDA 文档生成模型

按照文档生成的过程，使用贝叶斯估计统计学方法，将文档用多个主题来表示。LDA不只解决了同义词的问题，还解决了一次多义的问题。目前训练LDA模型的方法有原始论文中的基于EM和 差分贝叶斯方法以及后来出现的Gibbs Samplings 采样算法。

d)  Word2Vector 模型

最近几年刚刚火起来的算法，通过神经网络机器学习算法来训练N-gram 语言模型，并在训练过程中求出word所对应的vector的方法。本文将详细阐述此方法的原理。

###三，word2vec 的学习任务

假设有这样一句话：今天 下午 2点钟 搜索 引擎 组 开 组会。

任务1：对于每一个word， 使用该word周围的word 来预测当前word生成的概率。如使用“今天、下午、搜索、引擎、组”来生成“2点钟”。

任务2：对于每一个word，使用该word本身来预测生成其他word的概率。如使用“2点钟”来生成“今天、下午、搜索、引擎、组”中的每个word。

    两个任务共同的限制条件是：对于相同的输入，输出每个word的概率之和为1。

Word2vec的模型就是想通过机器学习的方法来达到提高上述任务准确率的一种方法。两个任务分别对应两个的模型（CBOW和skim-gram）。如果不做特殊说明，下文均使用CBOW即任务1所对应的模型来进行分析。Skim-gram模型分析方法相同。

###四，使用神经网络来训练

 

  此图是原始的未经过优化的用于训练上节提到的任务的神经网络模型。该模型有三层结构，输入层（投影层），隐藏层，输出层。

其中输入层和传统的神经网络模型有所不同，输入的每一个节点单元不再是一个标量值，而是一个向量，向量的每一个值为变量，训练过程中要对其进行更新，这个向量就是我们所关心的word所对应的vector，假设该向量维度为D。该层节点个数为整个语料库中的不同word的个数，设为V。

隐藏层和传统神经网络模型一样，使用激活函数为tanh 或sigmoid均可。假设该层节点个数为H。

输出层同样和传统神经网络模型一样，节点个数为整个语料库中的不同word的个数，即V。另外如果你认认真真的学过神经网络稀疏自编码的知识就会知道，如果将输入层和输出层之间再构建一层传递关系，更有利于提高该模型的准确度。

时间复杂度分析，对于上一节中所举得例子，我们知道输入是N个word，输出是“2点钟”。它所对应的时间复杂度为N * D + N * D * H + N *D * V + H * V。其中D和H大约在100和500之间，N是3到8，而V则高达几百万数量级。因此该模型的瓶颈在后面两项。接下来我们要讨论的问题就是如何解决这个时间复杂度的问题。

对于输入层到输出层的网络，我们可以直接将其剔除掉，实验证明，该措施并不会带来很多效果上的下降，反而省去了大部分计算时间。之所以隐藏层到输出层计算量最大是因为我们对于每一个输出的word均要进行验证并使用梯度下降更新。正确的word所对应的节点为正例，其他为反例，从而达到快速收敛的目的。下两节将介绍两种不同的方法来解决此问题。 

###五，Hierarchical Softmax

  如果我们将字典中的词分成2类，则在预测的时候我们可以先预测目标词所在的类别，这样就将直接去掉一半的测试，如果再在剩下的一半中再分成两类，则又可以去除一半的测试，一直分下去直到元素个数达两个为止。聪明的你，肯定想到了完全二叉树这个数据结构。没错！我们可以对字典建立一颗完全二叉平衡树，内部节点为分类节点，叶节点为代表每个词的节点。其中叶节点无需保存，没有实际意义，只需要保存V-1个内部节点，因为叶节点是由内部节点所确定的。

  更有意思的是，我们无需根据某些语义上的区别来分类每一个词，而是随意的进行分类，神经网络模型会自动的挖掘各个类别所代表的属性。

  因此，我们可以记录每个词到根节点的路径，然后每次只需对路径上的节点进行预测，并采用梯度下降的算法对神经网络的参数进行更新，就可以将复杂度从原来的H*V变为H*log（V）。如果考虑词频，适用Huffman 编码，则效率可再提升2到3倍。

###六，Negative Sampling

  首先，有一个非常重要的定理叫Noise Contrastive Estimation，是由Gutmann 和 Hyvarinen等人提出，之后被Mnih和The等人应用到语言模型中来。该定理的推倒证明已经超出本文所讲内容，有兴趣的读者可以自行阅读。

  应用到这里，只需要将各个词按照其对应的TF大小来随机选取并进行反例的更新。具体如下：将各个词的TF累和设为S，算出每个词的TF所占的比例，将长度为1的区间进行划分。之后随机产生一个数r（0<r<1），对应到长度为1的区间中，取出对应的word，并进行反例更新。

  总结起来就是，除了更新正确的word所对应的节点外，不在更新其他的所有反例节点，而是只选取几个反例节点进行更新。一般是10个左右。即复杂度变为10 * H。

###七，退变为多个logistic regression或Softmax模型

  Google research 等人又对上述模型进行改进，去除了隐藏层，重新拾起输入层到输出层的网络，并将原来的输入层起名投影层（projection layer），而且所有节点共享同一个投影层。重新加入原始的标量输入层节点表示每一个word。之后形成的神经网络和传统的神经网络几乎相同，唯一不同的是投影层没有激活函数，只有输出层有激活函数。同样使用上两节所用到的优化算法，我们可以得到以下模型。摘自网络

###八，加速方法总结

a) 删除隐藏层

b) 使用Hierarchical softmax 或negative sampling

c) 去除小于minCount的词

d) 根据一下公式算出每个词被选出的概率，如果选出来则不予更新。此方法可以节省时间而且可以提高非频繁词的准确度。

其中t为设定好的阈值，f（w） 为w出现的频率。

e) 选取邻近词的窗口大小不固定。有利于更加偏重于离自己近的词进行更新。

f)  多线程，无需考虑互斥。

###九，特性

Vectors(man) – vectors(woman) + vectors(daughters) = vectors(son)

LDA等其他模型无此性质。
	




##opencv

###获取人脸位置

###直方图均衡化
 直方图均衡化的英文名称是Histogram Equalization.
　　图像对比度增强的方法可以分成两类:一类是直接对比度增强方法;另一类是间接对比度增强方法。直方图拉伸和直方图均衡化是两种最常见的间接对比度增强方法。直方图拉伸是通过对比度拉伸对直方图进行调整，从而“扩大”前景和背景灰度的差别，以达到增强对比度的目的，这种方法可以利用线性或非线性的方法来实现;直方图均衡化则通过使用累积函数对灰度值进行“调整”以实现对比度的增强。
　　直方图均衡化处理的“中心思想”是把原始图像的灰度直方图从比较集中的某个灰度区间变成在全部灰度范围内的均匀分布。直方图均衡化就是对图像进行非线性拉伸，重新分配图像像素值，使一定灰度范围内的像素数量大致相同。直方图均衡化就是把给定图像的直方图分布改变成“均匀”分布直方图分布。
　　缺点：
　　1）变换后图像的灰度级减少，某些细节消失；
　　2）某些图像，如直方图有高峰，经处理后对比度不自然的过分增强。
　　直方图均衡化是图像处理领域中利用图像直方图对对比度进行调整的方法。
　　这种方法通常用来增加许多图像的局部对比度，尤其是当图像的有用数据的对比度相当接近的时候。通过这种方法，亮度可以更好地在直方图上分布。这样就可以用于增强局部的对比度而不影响整体的对比度，直方图均衡化通过有效地扩展常用的亮度来实现这种功能。
　　这种方法对于背景和前景都太亮或者太暗的图像非常有用，这种方法尤其是可以带来X光图像中更好的骨骼结构显示以及曝光过度或者曝光不足照片中更好的细节。这种方法的一个主要优势是它是一个相当直观的技术并且是可逆操作，如果已知均衡化函数，那么就可以恢复原始的直方图，并且计算量也不大。这种方法的一个缺点是它对处理的数据不加选择，它可能会增加背景杂讯的对比度并且降低有用信号的对比度。
　　直方图均衡化的基本思想是把原始图的直方图变换为均匀分布的形式，这样就增加了象素灰度值的动态范围从而可达到增强图像整体对比度的效果。设原始图像在(x，y)处的灰度为f，而改变后的图像为g，则对图像增强的方法可表述为将在(x，y)处的灰度f映射为g。在灰度直方图均衡化处理中对图像的映射函数可定义为:g = EQ (f)，这个映射函数EQ(f)必须满足两个条件(其中L为图像的灰度级数):
　　(1)EQ(f)在0≤f≤L-1范围内是一个单值单增函数。这是为了保证增强处理没有打乱原始图像的灰度排列次序，原图各灰度级在变换后仍保持从黑到白(或从白到黑)的排列。
　　(2)对于0≤f≤L-1有0≤g≤L-1，这个条件保证了变换前后灰度值动态范围的一致

性。
　　累积分布函数(cumulative distribution function，CDF)即可以满足上述两个条件，并且通过该函数可以完成将原图像f的分布转换成g的均匀分布。此时的直方图均衡化映射函数为:
　　gk = EQ(fk) = (ni/n) = pf(fi) ，
　　(k=0，1，2，……，L-1)
　　上述求和区间为0到k，根据该方程可以由源图像的各像素灰度值直接得到直方图均衡化后各像素的灰度值。在实际处理变换时，一般先对原始图像的灰度情况进行统计分析，并计算出原始直方图分布，然后根据计算出的累计直方图分布求出fk到gk的灰度映射关系。在重复上述步骤得到源图像所有灰度级到目标图像灰度级的映射关系后，按照这个映射关系对源图像各点像素进行灰度转换，即可完成对源图的直方图均衡化。



##optimizer：
###三、SGD（随机梯度下降优化器，性价比最好的算法）

    keras.optimizers.SGD(lr=0.01, momentum=0., decay=0., nesterov=False)  

     参数：

    lr :float>=0，学习速率
    momentum :float>=0 参数更新的动量
    decay : float>=0 每次更新后学习速率的衰减量
    nesterov :Boolean 是否使用Nesterov动量项

###四、Adagrad（参数推荐使用默认值） 

    keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)  

     参数：

    lr : float>=0，学习速率
    epsilon :float>=0

###五、Adadelta（参数推荐使用默认值）

    keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-6)  

     参数：

    lr :float>=0，学习速率
    rho : float>=0
    epsilon :float>=0 模糊因子

###六、RMSprop（参数推荐使用默认值）

    keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)  

    参数：

    lr:float>=0，学习速率
    rho : float>=0
    epsilon : float>=0 模糊因子

###七、Adam（参数推荐使用默认值）

    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)  

     参数：

    lr : float>=0，学习速率
    beta_1, beta_2:floats, 0 < beta < 1 通常都是接近于1
    epsilon :float>=0 模糊因子 


###激活函数：sigmoid，relu等

 论文参考：Deep Sparse Rectifier Neural Networks (很有趣的一篇paper）
起源：传统激活函数、脑神经元激活频率研究、稀疏激活性
传统Sigmoid系激活函数

技术分享

传统神经网络中最常用的两个激活函数，Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid）被视为神经网络的核心所在。

从数学上来看，非线性的Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。

从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。

无论是哪种解释，看起来都比早期的线性激活函数(y=x),阶跃激活函数(-1/1,0/1)高明了不少。
近似生物神经激活函数：Softplus&ReLu 

2001年，神经科学家Dayan、Abott从生物学角度，模拟出了脑神经元接受信号更精确的激活模型，该模型如左图所示：
技术分享
技术分享

技术分享技术分享

这个模型对比Sigmoid系主要变化有三点：①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性（重点，可以看到红框里前端状态完全没有激活）

同年，Charles Dugas等人在做正数回归预测论文中偶然使用了Softplus函数，Softplus函数是Logistic-Sigmoid函数原函数。

 Softplus(x)=log(1+ex)

按照论文的说法，一开始想要使用一个指数函数（天然正数）作为激活函数来回归，但是到后期梯度实在太大，难以训练，于是加了一个log来减缓上升趋势。

加了1是为了保证非负性。同年，Charles Dugas等人在NIPS会议论文中又调侃了一句，Softplus可以看作是强制非负校正函数max(0,x)平滑版本。

偶然的是，同是2001年，ML领域的Softplus/Rectifier激活函数与神经科学领域的提出脑神经元激活频率函数有神似的地方，这促成了新的激活函数的研究。
生物神经的稀疏激活性

在神经科学方面，除了新的激活频率函数之外，神经科学家还发现了神经元的稀疏激活性。

还是2001年，Attwell等人基于大脑能量消耗的观察学习上，推测神经元编码工作方式具有稀疏性和分布性。

2003年Lennie等人估测大脑同时被激活的神经元只有1~4%，进一步表明神经元工作的稀疏性。

从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。

从这个角度来看，在经验规则的初始化W之后，传统的Sigmoid系函数同时近乎有一半的神经元被激活，这不符合神经科学的研究，而且会给深度网络训练带来巨大问题。

Softplus照顾到了新模型的前两点，却没有稀疏激活性。因而，校正函数max(0,x)成了近似符合该模型的最大赢家。

 
Part I：关于稀疏性的观点

Machine Learning中的颠覆性研究是稀疏特征，基于数据的稀疏特征研究上，派生了Deep Learning这一分支。

稀疏性概念最早由Olshausen、Field在1997年对信号数据稀疏编码的研究中引入，并最早在卷积神经网络中得以大施拳脚。

近年来，稀疏性研究不仅在计算神经科学、机器学习领域活跃，甚至信号处理、统计学也在借鉴。

总结起来稀疏性大概有以下三方面的贡献：
1.1 信息解离

当前，深度学习一个明确的目标是从数据变量中解离出关键因子。原始数据（以自然数据为主）中通常缠绕着高度密集的特征。原因

是这些特征向量是相互关联的，一个小小的关键因子可能牵扰着一堆特征，有点像蝴蝶效应，牵一发而动全身。

基于数学原理的传统机器学习手段在解离这些关联特征方面具有致命弱点。

然而，如果能够解开特征间缠绕的复杂关系，转换为稀疏特征，那么特征就有了鲁棒性（去掉了无关的噪声）。
1.2 线性可分性

稀疏特征有更大可能线性可分，或者对非线性映射机制有更小的依赖。因为稀疏特征处于高维的特征空间上（被自动映射了）

从流形学习观点来看（参见降噪自动编码器），稀疏特征被移到了一个较为纯净的低维流形面上。

线性可分性亦可参照天然稀疏的文本型数据，即便没有隐层结构，仍然可以被分离的很好。
1.3 稠密分布但是稀疏

稠密缠绕分布着的特征是信息最富集的特征，从潜在性角度，往往比局部少数点携带的特征成倍的有效。

而稀疏特征，正是从稠密缠绕区解离出来的，潜在价值巨大。
1.4 稀疏性激活函数的贡献的作用：

不同的输入可能包含着大小不同关键特征，使用大小可变的数据结构去做容器，则更加灵活。

假如神经元激活具有稀疏性，那么不同激活路径上：不同数量（选择性不激活）、不同功能（分布式激活），

两种可优化的结构生成的激活路径，可以更好地从有效的数据的维度上，学习到相对稀疏的特征，起到自动化解离效果。

 
Part II：基于稀疏性的校正激活函数
2.1 非饱和线性端

撇开稀疏激活不谈，校正激活函数max(0,x)，与Softplus函数在兴奋端的差异较大(线性和非线性)。

几十年的机器学习发展中，我们形成了这样一个概念：非线性激活函数要比线性激活函数更加先进。

尤其是在布满Sigmoid函数的BP神经网络，布满径向基函数的SVM神经网络中，往往有这样的幻觉，非线性函数对非线性网络贡献巨大。

该幻觉在SVM中更加严重。核函数的形式并非完全是SVM能够处理非线性数据的主力功臣（支持向量充当着隐层角色）。

那么在深度网络中，对非线性的依赖程度就可以缩一缩。另外，在上一部分提到，稀疏特征并不需要网络具有很强的处理线性不可分机制。

综合以上两点，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。

技术分享

如图，一旦神经元与神经元之间改为线性激活，网络的非线性部分仅仅来自于神经元部分选择性激活。
2.2 Vanishing Gradient Problem

更倾向于使用线性神经激活函数的另外一个原因是，减轻梯度法训练深度网络时的Vanishing Gradient Problem。

看过BP推导的人都知道，误差从输出层反向传播算梯度时，在各层都要乘当前层的输入神经元值，激活函数的一阶导数。

即Grad=Error⋅Sigmoid′(x)⋅x。使用双端饱和(即值域被限制)Sigmoid系函数会有两个问题：

①Sigmoid‘(x)∈(0,1)  导数缩放

②x∈(0,1)或x∈(-1,1)  饱和值缩放

这样，经过每一层时，Error都是成倍的衰减，一旦进行递推式的多层的反向传播，梯度就会不停的衰减，消失，使得网络学习变慢。

而校正激活函数的梯度是1，且只有一端饱和，梯度很好的在反向传播中流动，训练速度得到了很大的提高。

Softplus函数则稍微慢点，Softplus‘(x)=Sigmoid(x)∈(0,1) ，但是也是单端饱和，因而速度仍然会比Sigmoid系函数快。

 
Part III 潜在问题
强制引入稀疏零的合理性？

诚然，稀疏性有很多优势。但是，过分的强制稀疏处理，会减少模型的有效容量。即特征屏蔽太多，导致模型无法学习到有效特征。

论文中对稀疏性的引入度做了实验，理想稀疏性（强制置0）比率是70%~85%。超过85%，网络就容量就成了问题，导致错误率极高。

技术分享

 

对比大脑工作的95%稀疏性来看，现有的计算神经网络和生物神经网络还是有很大差距的。

庆幸的是，ReLu只有负值才会被稀疏掉，即引入的稀疏性是可以训练调节的，是动态变化的。

只要进行梯度训练，网络可以向误差减少的方向，自动调控稀疏比率，保证激活链上存在着合理数量的非零值。

 
Part IV ReLu的贡献
4.1 缩小做和不做非监督预训练的代沟

ReLu的使用，使得网络可以自行引入稀疏性。这一做法，等效于无监督学习的预训练。

技术分享

当然，效果肯定没预训练好。论文中给出的数据显示，没做预训练情况下，ReLu激活网络遥遥领先其它激活函数。

甚至出现了比普通激活函数预训练后更好的奇葩情况。当然，在预训练后，ReLu仍然有提升空间。

从这一层面来说，ReLu缩小了非监督学习和监督学习之间的代沟。当然，还有更快的训练速度。
4.2 更快的特征学习

在MNIST+LeNet4中，ReLu+Tanh的组合在epoch 50左右就能把验证集错误率降到1.05%

但是，全Tanh在epoch 150时，还是1.37%，这个结果ReLu+Tanh在epoch 17时就能达到了。

技术分享

该图来自AlexNet的论文对ReLu和普通Sigmoid系函数做的对比测试，可以看到，ReLu的使用，使得学习周期

大大缩短。综合速率和效率，DL中大部分激活函数应该选择ReLu。


##CNN

##稀疏连接(Sparse Connectivity)

##权值共享(Shared Weights)

##RNN

##LSTM


##Inception


##tensorflow:


##opencv的基本操作
